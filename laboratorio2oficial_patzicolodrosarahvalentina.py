# -*- coding: utf-8 -*-
"""Laboratorio2oficial-PatziColodroSarahValentina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJ7CmJS5CuRRMrcmHivMbtiYuZSzXgXm

**Nombre:** Sarah Valentina Patzi Colodro

**Carrera:** Ingeniería en diseño y animación digital


**Para este laboratorio implementaremos y compararemos modelos de regresión a partir de un dataset que cumpla los requisitos de 10 columnas y 10000 filas**

**Dataset usado:** https://www.kaggle.com/datasets/stealthtechnologies/predict-purity-and-price-of-honey/data

**Nombre del Dataset:** Predict Purity and Price of Honey

**Formato:** 11 Columnas (solo se usaran 10 de las 11 debido a que la columna Pollen_analysis es texto ) 200K+ Filas (se usaran 17000)

**Variables de entrada:**

1.   **CS (Color Score):**
representa la puntuación de color de la muestra de miel, que va de 1,0 a 10,0. Los valores más bajos indican un color más claro, mientras que los valores más altos indican un color más oscuro.

2.   **Density:**
Representa la densidad de la muestra de miel en gramos por centímetro cúbico a 25°C, oscilando entre 1,21 y 1,86.

3. **WC (Water Content):**
Representa el contenido de agua en la muestra de miel, que varía entre 12,0% y 25,0%.

4. **pH:**
Representa el nivel de pH de la muestra de miel, que varía entre 2,50 y 7,50.

5. **EC (Electrical Conductivity):**
Representa la conductividad eléctrica de la muestra de miel en miliSiemens por centímetro.

6. **F (Fructose Level):**
Representa el nivel de fructosa de la muestra de miel, que varía de 20 a 50.

7. **G (Glucose Level):**
Representa el nivel de glucosa de la muestra de miel, que varía entre 20 y 45.

8. **Viscosity:**
Representa la viscosidad de la muestra de miel en centipoise, oscilando entre 1500 y 10000. Los valores de viscosidad entre 2500 y 9500 se consideran óptimos para la pureza.

9. **Purity:**
La variable objetivo representa la pureza de la muestra de miel, que varía entre 0,01 y 1,00.

**Variable de salida Y:**
* **Price (Precio):**
El precio calculado de la miel.

### **Importar Librerias**

**import os:**
Importamos os para manejar archivos y directorios.

**import numpy as np:**
Importamos numpy para operaciones matemáticas y manejo de arreglos.

**from matplotlib import pyplot:**
Importamos pyplot de matplotlib para crear gráficos.

**%matplotlib inline:**
Configuramos para que los gráficos se muestren dentro del cuaderno.
"""

# Commented out IPython magic to ensure Python compatibility.
# utilizado para manejos de directorios y rutas
import os

# Computacion vectorial y cientifica para python
import numpy as np

# Librerias para graficación (trazado de gráficos)
from matplotlib import pyplot

# Biblioteca para la manipulación y el análisis de datos
import pandas as pd

# llama a matplotlib a embeber graficas dentro de los cuadernillos
# %matplotlib inline

"""### **Importar carpetas de drive**"""

from google.colab import drive
drive.mount("/content/gdrive")

"""# **Regresion Lineal Multivariable**

### Cargar Datos:

Cargamos el dataset de pureza de miel desde un archivo CSV y eliminamos la columna Pollen_analysis porque contiene texto, lo cual no es útil para el análisis de regresión que requiere datos numéricos
"""

# Cargamos el dataset
data = pd.read_csv('/content/gdrive/MyDrive/IA/LaboratoriosOficiales/Lab2-PatziColodroSarahValentina/honey_purity_dataset.csv', delimiter=',')


# Eliminamos la columna 'Pollen_analysis' para que no interfiera en el análisis de regresión al ser texto y no numeros
data = data.drop('Pollen_analysis', axis=1)

#FORMA B: Seleccionamos las primeras 17,000 filas en orden, para poder visualizar de mejor manera las graficas ya que el dataset original cuenta con 247903 datos
#data = data.iloc[:17000]

print(data.shape)

data.info() #Muestra informacion si hay valores nulos y el tipo de datos de cada columna

"""### Ajustamos el tamaño de las filas y definimos columnas:

Seleccionamos las primeras 17,000 filas del dataset para facilitar la visualización de gráficos debido al tamaño grande del conjunto de datos original. Luego, separamos los datos en dos partes: "X", que contiene todas las columnas excepto la última (para las características), y "y", que toma solo la última columna (para la variable objetivo). "m" almacena el número de ejemplos (filas) en "y". Imprimimos las primeras 10 filas del DataFrame para verificar que los datos se cargaron correctamente y mostramos el DataFrame completo de manera visualmente formateada para una revisión más fácil.

* **[:17000,]** estás seleccionando desde la fila 0 hasta la fila 16,999

* **[:-1]:** Esto selecciona todas las columnas excepto la última

* **[-1]:** Aquí se selecciona solo la última columna del DataFrame
"""

# FORMA B: Seleccionamos las columnas independientes (X) y dependiente (y)
#X = data.iloc[:, :-1]  # Todas las columnas excepto la última [FILAS, COLUMNAS]
#y = data.iloc[:, -1]   # Solo la última columna [FILAS, COLUMNAS]

# Seleccionamos las primeras 17,000 filas en orden, para poder visualizar de mejor manera las graficas ya que el dataset original cuenta con 247903 datos
# tambien especificamos que para x queremos todas las columnas menos la ultima y para "Y" queremos la ultima
X = data.iloc[:17000, :-1]  #[FILAS, COLUMNAS]
y = data.iloc[:17000, -1]
m = y.size

# Verifica el contenido del array 'data'
print(data.head(10))  # Imprime las primeras 10 filas para asegurarte de que está correcto

print('Número de ejemplos (filas): ', m)

#se utiliza para mostrar un DataFrame de pandas de manera más visual y formateada
display(data)

"""### Cargar 100 filas posteriores a las primeras 17000:

Seleccionamos las filas 17,000 a 17,100 para obtener 100 datos adicionales que se usarán posteriormente en las predicciones.
Estos 100 valores están separados de las 17,000 filas iniciales que usamos previamente, proporcionando un subconjunto específico para ajustar probar el modelo.
"""

# Seleccionamos las filas 17,000 a 17,100 para las características y la variable de salida
X_prediccion = data.iloc[17000:17100, :-1]  # Características de las filas adicionales
y_prediccion = data.iloc[17000:17100, -1]   # Variable de salida de las filas adicionales

# Verifica el contenido de las filas adicionales
#print(X_prediccion.head(10))  # Imprime las primeras 10 filas de X_prediccion
#print(y_prediccion.head(10))  # Imprime las primeras 10 filas de y_prediccion

"""### Verifica si hay valores nulos en X:"""

print(np.isnan(X).sum())

"""### **Visualizar de otras formas:**

Imprimimos los nombres de las 9 columnas de características (X) junto con la columna de salida (y) para asegurarnos de que todo esté claro. Luego, usamos un bucle para mostrar los primeros 10 valores de cada columna en X y el valor correspondiente en y. Esto nos ayuda a verificar visualmente los datos y confirmar que las columnas están alineadas correctamente con sus valores.
"""

# Supongamos que tienes 9 columnas en X más la columna y
print('{:>5s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}'.format('X[:,0]','X[:,1]','X[:,2]','X[:,3]','X[:,4]','X[:,5]','X[:,6]','X[:,7]','X[:,8]', 'y')) # Imprime los nombres de las columnas

# Ajusta la cantidad de valores que imprimes dentro del loop para que coincida con la cantidad de columnas en X
for i in range(10):
    print('{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}'.format(
        X.iloc[i, 0], X.iloc[i, 1], X.iloc[i, 2], X.iloc[i, 3], X.iloc[i, 4], X.iloc[i, 5], X.iloc[i, 6], X.iloc[i, 7], X.iloc[i, 8], y.iloc[i]
    ))

"""Muestra las primeras 5 filas de X"""

print(X.head())

"""### **Normalizar Caracteristicas**
 Este es un paso importante en el preprocesamiento de datos en aprendizaje automático, transformamos las caracteristicas a valores similares para hacer que converjan mas rápido
"""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# Llama a la funcion featureNormalize con el parametro X para obtener los datos normalizados
X_norm, mu, sigma = featureNormalize(X)

print('Media calculada:\n', mu)
print('Desviación estandar calculada:\n', sigma)
print('Datos normalizados:\n',X_norm) # Muestra los datos normalizados

"""Añadimos una columna de unos como primera columna en X, el termino de sesgo es un valor constante que se añade a la ecuación de predicción para ajustar la línea de regresión de manera que se alinee mejor con los datos observados

### **Graficar las caracteristicas:**

Definimos la función graficarDatos para crear gráficos de dispersión con los datos proporcionados. La función toma x y y como los datos para el gráfico, y xlabel y ylabel para las etiquetas de los ejes. Luego, creamos gráficos de dispersión para cada característica normalizada (X_norm) comparada con el precio de la miel (y). Esto nos permite visualizar cómo cada característica se relaciona con el precio de la miel y entender mejor los datos.

* x: Datos que se mostrarán en el eje horizontal (eje x) del gráfico. En este caso, corresponde a una de las características normalizadas de X_norm.

* y: Datos que se mostrarán en el eje vertical (eje y) del gráfico. Aquí, es el precio de la miel.

* xlabel: Etiqueta para el eje horizontal (eje x). Describe qué representa la variable en el eje x, como "CS (Puntuación de color)".

* ylabel: Etiqueta para el eje vertical (eje y). Describe qué representa la variable en el eje y, en este caso, el precio de la miel.
"""

def graficarDatos(x, y, xlabel, ylabel):
  fig = pyplot.figure()
  pyplot.plot(x, y, 'ro')
  pyplot.xlabel(xlabel)
  pyplot.ylabel(ylabel)

xlabel1 = "CS(Puntuación de color)"
xlabel2 = "Density(Densidad)"
xlabel3 = "WC(contenido de agua)"
xlabel4 = "Nivel de ph"
xlabel5 = "EC(conductividad eléctrica)"
xlabel6 = "F(Fructosa)"
xlabel7 = "G(Glucosa)"
xlabel8 = "Viscosity(Viscosidad)"
xlabel9 = "Purity(Pureza)"
ylabel1 = "Price(Precio de la miel)"
graficarDatos(X_norm['CS'], y, xlabel1, ylabel1)
graficarDatos(X_norm['Density'], y, xlabel2, ylabel1)
graficarDatos(X_norm['WC'], y, xlabel3, ylabel1)
graficarDatos(X_norm['pH'], y, xlabel4, ylabel1)
graficarDatos(X_norm['EC'], y, xlabel5, ylabel1)
graficarDatos(X_norm['F'], y, xlabel6, ylabel1)
graficarDatos(X_norm['G'], y, xlabel7, ylabel1)
graficarDatos(X_norm['Viscosity'], y, xlabel8, ylabel1)
graficarDatos(X_norm['Purity'], y, xlabel9, ylabel1)

"""### **Se agrega un termino de interseccion:**

Agregamos una columna de 1 a los datos de entrada ya que nos permite representar el término como un parámetro independiente que multiplica a la columna de unos. Esto nos da la flexibilidad de ajustar el modelo para que se ajuste mejor a los datos
"""

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1) #Añadimos una columna de unos a X

print('Datos normalizados (primeras 10 filas):\n', X_norm[:10])
print('Media calculada:\n', np.mean(X_norm, axis=0))
print('Desviación estándar calculada:\n', np.std(X_norm, axis=0))

"""### **Cálculo del costo $J(\theta)$:**

Esta funcion calcula el costo, el costo mide la diferencia entre las predicciones realizadas por el modelo y los valores reales observados en los datos.
"""

def computeCostMulti(X, y, theta):
    # almacenamos en m la cantidad de filas que tiene y
    m = y.shape[0]
    # variable que almacenara el valor del costo
    J = 0
    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))
    return J

"""### **Descenso por el gradiente:**

**Funcion descenso de la gradiente**, ajusta los parametros theta del modelo de regresion de manera que se minimice la funcion de costo
"""

def gradientDescentMulti(X, y, theta, alpha, num_iters):
    m = y.shape[0] # almacenamos en m la cantidad de filas que tiene "y"
    theta = theta.copy() # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente
    J_history = [] # una lista que almacenara el valor de la funcion de costo en cada iteracion
    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)

        # Después de actualizar theta, se calcula el costo actual con computeCostMulti y se guarda en J_history.
        J_history.append(computeCostMulti(X, y, theta))
    return theta, J_history

"""### Seleccionar coheficientes de aprendizaje para hacer la predicción"""

# Elegir algun valor para alpha
alpha = 0.001
num_iters = 10000

# inicializa theta y ejecuta el descenso por el gradiente:
#theta[0] es el peso asociado con la columna de unos (el sesgo). theta[1] a theta[9] son los pesos asociados con las 9 características de entrada.

theta = np.zeros(10) # son solo las 9 columnas de X más 1 que aumentamos de unos (sesgo)
theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)

# Grafica la convergencia del costo
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')
pyplot.title('Convergencia del Descenso por el Gradiente')
pyplot.show()

# Muestra los resultados del descenso por el gradiente
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))

print('Theta encontrada por descenso gradiente: {:.4f}, {:.4f}'.format(*theta))
print(f"Costo final después de {num_iters} iteraciones: {J_history[-1]:.2f}")

"""## **PREDICCIONES DE LA REGRESION LINEAL MULTIVARIABLE**

**Calculamos la Y predicha**

Multiplicamos las caracteristicas con la theta y este proceso nos da una estimación de la variable de salida
"""

# Estimar el precio de la miel:
#CS(Puntuación de color)	Density(Densidad)	WC(contenido de agua)	pH	EC(conductividad eléctrica)	F(Fructosa)	G(Glucosa)	Viscosity(Viscosidad)	Purity(Pureza)
X_array = [1,2.81,1.75,23.04,6.29,0.76,39.02,33.63,4844.5,0.68] #El primer valor 1 corresponde a la columna de unos que añadimos previamente a X

# Normalizar las características (excepto la columna de unos): X_array[1:10] excluye el primer elemento (X_array[0], que es el sesgo) y toma los siguientes 9 elementos,
#que corresponden a las 9 características de entrada.
X_array[1:10] = (X_array[1:10] - mu) / sigma

Precio = np.dot(X_array, theta) #645.24

print('El precio predecido de la miel es: ${:.2f}'.format(Precio))

"""### **100 Predicciones**

Realizo 100 predicciones con los 100 datos establecidos previamente en las variables **X_prediccion** y **y_prediccion**
"""

# Normaliza las características de las filas adicionales
X_prediccion_normalized = (X_prediccion - mu) / sigma

# Añadir el sesgo (columna de unos) a las características normalizadas
X_prediccion_normalized = np.hstack([np.ones((X_prediccion_normalized.shape[0], 1)), X_prediccion_normalized])

# Realiza las predicciones
predicciones = np.dot(X_prediccion_normalized, theta)

# Imprime las predicciones junto con el valor real
for i, (prediccion, real) in enumerate(zip(predicciones, y_prediccion), start=1):
    print(f"Predicción {i}: ${prediccion:.0f} - Valor Real: ${real:.0f}")

"""# **Ecuacion de la normal**

### **Importar Librerias**
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot

"""### **Cargar Datos:**
Cargamos el dataset de pureza de miel desde un archivo CSV y eliminamos la columna Pollen_analysis porque contiene texto, lo cual no es útil para el análisis de regresión que requiere datos numéricos
"""

# Cargamos el dataset
data = pd.read_csv('/content/gdrive/MyDrive/IA/LaboratoriosOficiales/Lab2-PatziColodroSarahValentina/honey_purity_dataset.csv', delimiter=',')


# Eliminamos la columna 'Pollen_analysis' para que no interfiera en el análisis de regresión al ser texto y no numeros
data = data.drop('Pollen_analysis', axis=1)

# Seleccionamos las primeras 17,000 filas en orden, para poder visualizar de mejor manera las graficas ya que el dataset original cuenta con 247903 datos
data = data.iloc[:17000]

print(data.shape)

"""Seleccionamos las columnas del DataFrame data para crear nuestras variables de características y salida. X contiene todas las columnas excepto la última (las características independientes), y y toma solo la última columna (la variable dependiente). Calculamos el número de ejemplos (filas) en y con y.size. Luego, imprimimos las primeras 10 filas de data para verificar que los datos se cargaron correctamente y usamos display(data) para mostrar el DataFrame de manera más visual y formateada."""

# Seleccionamos las columnas independientes (X) y dependiente (y)
X = data.iloc[:, :-1]  # Todas las columnas excepto la última
y = data.iloc[:, -1]   # Solo la última columna
m = y.size

# Verifica el contenido del array 'data'
print(data.head(10))  # Imprime las primeras 10 filas para asegurarte de que está correcto

print('Número de ejemplos (filas): ', m)

#se utiliza para mostrar un DataFrame de pandas de manera más visual y formateada
display(data)

print(X.head()) # Muestra las primeras 5 filas de X

"""### **Añadimos una columna de unos**
esta línea de código añade una columna de unos a X para incluir un término de sesgo en el modelo de regresión lineal. Este término de sesgo es necesario para calcular la intersección en la ecuación de la recta de regresión.
"""

X = np.concatenate([np.ones((m, 1)), X], axis=1)

"""### **Agregamos la funcion de la ecuación de la normal:**"""

# Funcion de la ecuacion de la normal, obtenemos valores optimos
def normalEqn(X, y):
    theta = np.zeros(X.shape[1])
    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)
    return theta

"""### **Hacemos el calculo de theta haciendo uso de la ecuación de la Normal:**"""

# Calcula los parametros con la ecuación de la normal
theta = normalEqn(X, y)

# Muestra los resultados obtenidos a partir de la aplicación de la ecuación de la normal
print('Theta calculado a partir de la ecuación de la normal: {:s}'.format(str(theta)))
X_array = [1,2.81,1.75,23.04,6.29,0.76,39.02,33.63,4844.5,0.68]
price = np.dot(X_array, theta)

print('Precio de miel (usando la ecuación de la normal): ${:.3f}'.format(price))

"""### Prediccion inicial:"""

# Prediccion 1 ecuacion de la normal
X_array = [1,3.17, 1.60, 20.08, 7.00, 0.60, 30.78, 30.39, 4763.3, 0.70]
price = np.dot(X_array, theta)

print('Precio predecido : ${:.2f}'.format(price))

"""### **100 Predicciones de Ejemplo Ecuación de la Normal**

En este código, estamos generando ejemplos aleatorios para probar cómo cambia el precio de la miel basado en una ecuación de regresión.
* for i in range(100):
Ejecuta el bucle 100 veces para generar 100 ejemplos.

* X_array = np.random.rand(10) * np.random.randint(1, 2, size=10)
Crea un arreglo X_array de 10 números aleatorios. Cada número se genera multiplicando un valor aleatorio entre 0 y 1 por un número entero aleatorio (1 o 2).

* print("Ejemplo", i+1, ":", X_array)
Imprime el índice del ejemplo actual y el arreglo X_array generado.

* estado = np.dot(X_array, theta)
Calcula el precio de la miel usando la ecuación de la regresión lineal, donde theta es el vector de parámetros del modelo. La función np.dot realiza el producto punto entre X_array y theta.

* print('Precio de miel (usando la ecuación de la normal): {:.3f}'.format(estado))
Imprime el precio de la miel calculado con formato de 3 decimales.
"""

for i in range(100):
    X_array = np.random.rand(10) * np.random.randint(1, 2, size=10)
    print("Ejemplo", i+1, ":", X_array)
    estado = np.dot(X_array, theta)
    print('Precio de miel (usando la ecuación de la normal): {:.3f}'.format(estado))

"""# **REGRESION POLINOMICA**

### **Importar carpetas de drive**
"""

from google.colab import drive
drive.mount("/content/gdrive")

"""### **Importar Librerias**"""

# Commented out IPython magic to ensure Python compatibility.
# utilizado para manejos de directorios y rutas
import os

# Computacion vectorial y cientifica para python
import numpy as np

import pandas as pd

# Librerias para graficación (trazado de gráficos)
from matplotlib import pyplot
from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D

# llama a matplotlib a embeber graficas dentro de los cuadernillos
# %matplotlib inline

"""### Cargar Datos:"""

# Cargamos el dataset
data = pd.read_csv('/content/gdrive/MyDrive/IA/LaboratoriosOficiales/Lab2-PatziColodroSarahValentina/honey_purity_dataset.csv', delimiter=',')


# Eliminamos la columna 'Pollen_analysis' para que no interfiera en el análisis de regresión al ser texto y no numeros
data = data.drop('Pollen_analysis', axis=1)

#FORMA B: Seleccionamos las primeras 17,000 filas en orden, para poder visualizar de mejor manera las graficas ya que el dataset original cuenta con 247903 datos
#data = data.iloc[:17000]

print(data.shape)

"""### Ajustamos el tamaño de las filas y definimos columnas:"""

# Seleccionamos las primeras 17,000 filas en orden, para poder visualizar de mejor manera las graficas ya que el dataset original cuenta con 247903 datos
# tambien especificamos que para x queremos todas las columnas menos la ultima y para "Y" queremos la ultima
X = data.iloc[:17000, :-1]  #[FILAS, COLUMNAS]
y = data.iloc[:17000, -1]
m = y.size

# Verifica el contenido del array 'data'
print(data.head(10))  # Imprime las primeras 10 filas para asegurarte de que está correcto

print('Número de ejemplos (filas): ', m)

#se utiliza para mostrar un DataFrame de pandas de manera más visual y formateada
display(data)

"""### **Cargar 100 filas posteriores a las primeras 17000:**"""

# Seleccionamos las filas 17,000 a 17,100 para las características y la variable de salida
X_prediccion = data.iloc[17000:17100, :-1]  # Características de las filas adicionales
y_prediccion = data.iloc[17000:17100, -1]   # Variable de salida de las filas adicionales

def plotData(x, y):
    #Grafica los puntos x e y en una figura nueva.

    fig = pyplot.figure()  # abre una nueva figura

    pyplot.plot(x, y, 'ro', ms=10, mec='k')
    pyplot.ylabel('Precio')
    pyplot.xlabel('Caracteristica')

"""### **Expansión de Características en Regresión Polinómica:**

Añade una nueva característica al conjunto de datos X para hacer una regresión polinómica. La línea X = np.concatenate([X, X * X], axis=1) toma la matriz X y la amplía añadiendo una nueva columna que es el cuadrado de cada valor en X
"""

X = np.concatenate([X, X * X], axis=1)
print(X)

"""### **Normalización de Características:**

La función featureNormalize ajusta las características de X para que tengan una media de 0 y una desviación estándar de 1. Calcula la media y desviación estándar de cada columna, luego ajusta los valores de X restando la media y dividiendo por la desviación estándar. Devuelve el conjunto de datos normalizado, junto con las medias y desviaciones estándar usadas.
"""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# llama featureNormalize con los datos cargados
X_norm, mu, sigma = featureNormalize(X)
print(X_norm)

"""### **Añade un Término de Sesgo:**

Este código agrega una columna de unos al inicio del conjunto de datos X_norm para incluir un término de sesgo en el modelo
"""

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
print(X)

"""### **DESCENSO POR EL GRADIENTE:**

### **Calculo de costo:**

La función computeCostMulti calcula el costo del modelo de regresión multivariable comparando las predicciones con los valores reales. Usa la fórmula del error cuadrático medio para medir qué tan bien se ajusta el modelo.
"""

def computeCostMulti(X, y, theta):
    # Inicializa algunos valores utiles
    m = y.shape[0] # numero de ejemplos de entrenamiento

    J = 0

    h = np.dot(X, theta)

    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))

    return J

"""### **Descenso por el gradiente:**

La función gradientDescentMulti ajusta los parámetros del modelo theta para minimizar el costo usando el descenso por el gradiente. Actualiza theta iterativamente, calculando el gradiente y restándolo de theta. Guarda el costo en cada iteración para rastrear el progreso. Finalmente, devuelve los parámetros ajustados y la historia del costo.
"""

def gradientDescentMulti(X, y, theta, alpha, num_iters):

    # Inicializa algunos valores
    m = y.shape[0] # numero de ejemplos de entrenamiento

    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente
    theta = theta.copy()

    J_history = []

    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        J_history.append(computeCostMulti(X, y, theta))

    return theta, J_history

"""### **Configuración y Ejecución del Descenso por el Gradiente:**

Se establece una tasa de aprendizaje alpha y el número de iteraciones. Se inicializan los parámetros theta y se ejecuta el descenso por el gradiente con gradientDescentMulti. Luego, se grafica la evolución del costo a lo largo de las iteraciones y se muestra el valor final de theta.
"""

# Elegir algun valor para alpha (probar varias alternativas)
alpha = 0.001
num_iters = 10000

# inicializa theta y ejecuta el descenso por el gradiente
theta = np.zeros(19) # son solo las 9 columnas (9*2 =18) de X más 1 que aumentamos de unos (sesgo)
theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)

# Grafica la convergencia del costo
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

# Muestra los resultados del descenso por el gradiente
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))

"""## **PREDICCIONES**

Primero, se normalizan las características de X_array (excepto el primer valor de sesgo) usando mu y sigma. Luego, se usa el modelo theta para calcular el precio estimado a partir de X_array mediante la multiplicación de matrices. Finalmente, se imprimen el valor de theta encontrado, el costo final después de las iteraciones y el precio estimado.
"""

X_array = [1, 2.81, 1.75, 23.04, 6.29, 0.76, 39.02, 33.63, 4844.5, 0.68,
          7.8961, 3.0625, 531.3024, 39.5641, 0.5776, 1521.3604, 1123.5969, 23458700.25, 0.4624] #las primeras 9 características después del 1 son las originales, y las 9 características adicionales al final son sus cuadrados
X_array[1:19] = (X_array[1:19] - mu) / sigma
price = np.dot(X_array, theta)

print('Theta encontrada por descenso gradiente: {:.4f}, {:.4f}'.format(*theta))
print(f"Costo final después de {num_iters} iteraciones: {J_history[-1]:.2f}")

print('El precio (usando el descenso por el gradiente): ${:.0f}'.format(price))

"""### 100 predicciones

Primero, se toma X_prediccion, que son las características de las filas 17,000 a 17,100 del dataset, y se les añaden términos cuadrados para capturar relaciones no lineales. Luego, se normalizan estos datos usando las medias (mu) y desviaciones estándar (sigma) calculadas en los datos de entrenamiento. Se agrega una columna de unos para incluir el término de sesgo, generando X_prediccion_nor_ses. Usando el modelo theta, se calculan las predicciones (predicciones_RegPol). Finalmente, se imprimen estas predicciones junto con los valores reales esperados (y_prediccion) para comparar y evaluar el rendimiento del modelo.
"""

# Agregar términos al cuadrado
X_prediccion_Cuadrado = np.concatenate([X_prediccion, X_prediccion ** 2], axis=1)

# Normalización de los datos de prueba usando mu y sigma calculados en los datos de entrenamiento
X_prediccion_Cuadrado_norm = (X_prediccion_Cuadrado - mu) / sigma

# Agregar la columna de unos a los datos normalizados
X_prediccion_nor_ses = np.concatenate([np.ones((X_prediccion_Cuadrado_norm.shape[0], 1)), X_prediccion_Cuadrado_norm], axis=1)

# Realizar las predicciones
predicciones_RegPol = np.dot(X_prediccion_nor_ses, theta)

# Imprimir las predicciones junto con los valores reales
for i, (prediccion, real) in enumerate(zip(predicciones_RegPol, y_prediccion)):
    print(f'Predicción {i+1}: ${prediccion:.0f} - Valor Esperado: ${real:.0f}')